ğŸ“¦ CNN from Scratch (NumPy + MNIST)

This project implements a **Convolutional Neural Network (CNN)** entirely from scratch using only **NumPy**, without using any deep learning frameworks like TensorFlow or PyTorch.

It is trained on the **MNIST handwritten digits dataset** and achieves over **83% test accuracy** after just 3 epochs on a 1000-image subset. This project demonstrates a fundamental understanding of how CNNs work internally â€” including forward pass, backpropagation, and training logic.

---

## ğŸ“š Table of Contents

- [ğŸ” Project Highlights](#-project-highlights)
- [ğŸ§  Network Architecture](#-network-architecture)
- [ğŸ“Š Performance](#-performance)
- [ğŸ’» Run Locally](#-run-locally)
- [ğŸ› ï¸ Possible Improvements](#-possible-improvements)
- [ğŸ“ Learning Outcomes](#-learning-outcomes)
- [ğŸ“Œ Credits](#-credits)
- [ğŸ“¬ Contact](#-contact)
- [ğŸ·ï¸ License](#-license)

---

## ğŸ” Project Highlights

- âœ… Custom implementation of:
  - `Conv3x3` convolutional layer
  - `MaxPool2` max-pooling layer
  - `Softmax` classifier
- âœ… Manual backpropagation for all layers
- âœ… Cross-entropy loss + SGD training
- âœ… Data normalization and reshaping
- âœ… Trained on a 1000-sample subset of MNIST for demonstration speed
- âœ… Epoch-wise and batch-wise logging of loss and accuracy

---

## ğŸ§  Network Architecture

```text
Input:       28x28 grayscale image
â†“ Conv3x3    (8 filters)       â†’ Output: 26x26x8
â†“ MaxPool2   (2x2 pooling)     â†’ Output: 13x13x8
â†“ Flatten
â†“ Softmax    (Fully connected) â†’ Output: 10 class scores
```
---
ğŸ’» Run Locally

1. Clone the repository
  bash
```text
git clone https://github.com/yourusername/cnn-from-scratch.git
cd cnn-from-scratch
```
2. (Optional) Create and activate a virtual environment
  bash
```text
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```
3. Install dependencies
  bash
```text
pip install numpy tensorflow
```
4. Run the training script
  bash
```text
python cnn.py
```
You should see epoch-wise logs showing loss and accuracy.

ğŸ› ï¸ Possible Improvements

	â€¢	Add support for multiple convolution + pooling layers
	â€¢	Add dropout and batch normalization
	â€¢	Implement additional optimizers (Adam, RMSprop)
	â€¢	Train on the full MNIST dataset for better accuracy
	â€¢	Make the code modular and dataset-agnostic (e.g., CIFAR-10)
	â€¢	Visualize learned filters and activations
	â€¢	Add a CLI or training dashboard for visualization
 
 ğŸ“ Learning Outcomes
      
    This project helped me:
	
    â€¢	Understand how CNNs work internally
  	â€¢	Manually implement convolution, pooling, and softmax layers
  	â€¢	Build the forward and backward pass logic from scratch
  	â€¢	Train a neural network using raw NumPy
  	â€¢	Appreciate the foundations of deep learning without using frameworks

â¸»

ğŸ“Œ Credits

	â€¢	MNIST dataset is provided via tensorflow.keras.datasets
	â€¢	Inspired by multiple deep learning â€œfrom scratchâ€ resources and coursework

â¸»

ğŸ“¬ Contact

Feel free to reach out for suggestions, collaborations, or feedback:
	â€¢	ğŸ“§ Email: pandeydeepansh24@gmail.com
	â€¢	ğŸ”— LinkedIn: linkedin.com/in/deepansh-pandey-12235830a
